 ---
editor_options: 
  markdown: 
    wrap: sentence
---

```{r}
# We load all the librarys for this notebook
library(tidyverse) 
library(janitor)
library(VIM)

# Cargamos las funciones externas
source("../R/01_functions.R")
```

# 1. Exploratory Data Analysis & Preprocessing

## 1.1 Introducción y Estrategia de Datos

El objetivo de este proyecto es transformar datos médicos brutos en activos analíticos de alta fidelidad. A lo largo de este y los próximos notebooks, desplegaremos una estrategia integral de aprendizaje automático que abarca:

Aprendizaje No Supervisado: Evaluación de la estructura natural de los datos mediante K-means y DBSCAN.

Aprendizaje Supervisado: Modelado predictivo comparando Regresión Logística (baseline), C5.0 con matriz de costes (enfoque clínico) y Random Forest (potencia no lineal).

En lugar de un análisis aislado, enfrentaremos dos datasets con naturalezas estadísticas opuestas para poner a prueba la robustez de nuestro pipeline.

### A. Cardiovascular Disease Dataset (Mendeley Data)

Este conjunto de datos comprende **1,529 muestras** recolectadas entre enero de 2024 y enero de 2025.
Incluye una visión 360º del paciente a través de parámetros demográficos, antropométricos, clínicos y bioquímicos.

Hipótesis Analítica: El riesgo cardiovascular se manifiesta como un sistema complejo de factores interdependientes.
A diferencia de otras patologías, sospechamos que no existe una frontera nítida de diagnóstico, sino un continuum biológico.
Por ejemplo, un biomarcador alterado (como el colesterol) podría verse compensado por un estilo de vida protector (actividad física alta), difuminando los límites entre grupos de riesgo.

Expectativa: Basándonos en esta naturaleza sistémica, nuestra hipótesis de trabajo es que el dataset presentará una estructura de "nube" con un alto grado de solapamiento (Entropía). Esto supone un desafío estratégico para nuestro pipeline:

En Clustering: Esperamos que los algoritmos tengan dificultades para identificar fronteras físicas claras debido al solapamiento de los puntos. creemos que l modelo sera incapadz de crear islas claras debido al espectro continuum.

En Predicción (Supervisados): La ambigüedad de las fronteras exigirá el uso de modelos de clasificación robustos y técnicas de regularización. La Regresión Logística podría sufrir al intentar trazar una línea simple en un espacio tan mezclado y los arboles de decision lo tendrandificil para encontrar los pnutosde corte o ramificacion. Este dataset representa el "escenario de mundo real" donde la predicción precisa es un reto.

-   **Referencia:** [Mendeley Data - CVD Dataset](https://data.mendeley.com/datasets/d9scg7j8fp/1)

### B. Breast Cancer Wisconsin (Diagnostic) - UCI

Este dataset clínico contiene mediciones morfológicas obtenidas a partir de imágenes digitalizadas de una aspiración con aguja fina (FNA) de una masa mamaria de 569 pacientes. El dataset se compone de 30 variables numéricas continuas que describen características geométricas y de textura de los núcleos celulares de celulas tumorales (radio, perímetro, área, concavidad, etc.).

Hipótesis Analítica: En contraposición al caso cardiovascular (CVD), donde el riesgo es una condición sistémica y difusa, la malignidad celular en oncología presenta una firma física drástica y mensurable. Las células malignas suelen romper la simetría y las dimensiones de las células sanas, lo que debería traducirse en una estructura de datos con fronteras de decisión mucho más marcadas.

Expectativa: Creemos que este escenario presentará una separabilidad nítida debido a la alta correlación entre la morfología celular y el diagnóstico. Al depender de mediciones físicas precisas y no de sintomatología subjetiva, este dataset actuará como nuestro benchmark de validación:

En Clustering: Esperamos encontrar "islas" de datos o clústeres casi perfectamente definidos (Benign vs. Malignant) en el espacio proyectado por el PCA.

En Predicción (Supervisados): Al existir una diferencia biológica tan clara, Esperamos que la Regresión Logística alcance una precisión alta debido a la probable separabilidad lineal del dataset.ADEMAS, Tanto C5.0 como Random Forest deberían alcanzar métricas de excelencia (Accuracy y F1-Score >90%), validando que nuestro pipeline es capaz de detectar señales biológicas claras cuando estas existen.

Referencia: [UCI Machine Learning - Breast Cancer Wisconsin (Diagnostic)] (https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)


## 1.2 Data loading ang initial exploration

Cargamos ambos datasets desde el directorio data/raw/.
Con el dataset dew CVD todo bien (es un clasoico csv con las cabezeras) i es facil de cargar. El problema llega con el dataset de cancer, el cual, al ser antiguo, no es un .csv sino que es un .data. este, ademas, no tiene las cabeceras. Por tanto, en la funcion de carga, hemos tenido que declarar manualmente losnombres de cada varable (cabeceras) en base a la informacion de la pagina de la UCI mencionada antes. 

```{r}
# 1. Carga del Caos (CVD)
cvd_raw <- load_medical_data("../data/raw/CVD Dataset.csv")
# 2. Carga del Orden (Breast Cancer)
# La función detectará 'wdbc.data' y pondrá los 32 nombres automáticamente
cancer_raw <- load_medical_data("../data/raw/wdbc.data")

# Inspección rápida
head(cvd_raw, 3)
head(cancer_raw, 3)
```

Una práctica fundamental antes de cualquier proceso de limpieza es realizar una inspección visual de los valores posibles por cada atributo.
Esto nos permite identificar rápidamente la naturaleza de la variabele (discreta, continua...)

Extraeremos los 10 primeros valores únicos y válidos de cada variable para ambos escenarios médicos.
Empecemos con el de CDV:

```{r}
# Empecemos con el dataset de CDV
get_unique_valid_values(cvd_raw, 10)
```

Identificamos variables categóricas con etiquetas explícitas de texto (como sex: "F"/"M", smoking_status: "Y"/"N" o cvd_risk_level: "LOW"/"HIGH").Tambien vemos numericas, algunas enteras (age) o algunas decimadels (como bmi).
Destaca blood_pressure_mm_hg (ej. "125/79"), que es una cadena de texto que ya ha sido desglosada en el dataset original en systolic_bp y diastolic_bp

Pasemos con el de HCV:

```{r}
# 
get_unique_valid_values(cancer_raw, 10)
```

A diferencia del dataset de CVD, qui vems que  todas las variables son numericas (con excepcion de el que sera nuestro target "diagnosis"). Destaca ID que, al ser un valor de ... no es necesaro para identificar la diagnosis i po tanto debera ser retirado.


## 1.3 Diccionario de Variables

En base a lo anteior, i a lo extraido de las paginas de obtencion de los datasets (revisar enlaces anteriores), podemos establecer el diccionario:

### A. Cardiovascular Disease Dataset (CVD)

Este dataset es un conjuto de 1529 observaciones con 22 tanto cateoicas como numericas que describen diferentes caracteirsticas de los pacientes (en ambitos muy variados como demografia, habitos o parametro bioquimicos) i ofrecen el grado de riesgo cardiovascular (recodigo en dos variables que prepresentan la misma realidad: cvd_risk_level -siendo este 3 3posibles vcalores alto, medio i bajo- i cvd_risk _Score -siendo este una escala-). El hecho de que el riesgo se pueda expresar mediante un score continuo refuerza nuestra tesis inicial: el riesgo cardiovascular no es un interruptor binario, sino un continuum biológico.
Esta naturaleza numérica subyacente es la que anticipa la dificultad de los algoritmos para encontrar fronteras nítidas, ya que el paso de un nivel a otro es gradual y no discreto.
Auqneu ambas son candidatas a ser targeten los modelos supervisados decidimos seleccionar cvd_risk_level como variable objetivo por tres motivos:
- Permite comparar ambos datasets bajo un marco común de targets categoricos.
- Facilita el diseño de sistemas de triaje médico donde el error se gestiona mediante matrices de coste (ej. evitar falsos negativos en riesgo alto).
- Actúa como ground truth para medir si el aprendizaje no supervisado es capaz de identificar fronteras en datos altamente solapados. un dato continuo no permite evaluar lo discreto de x islas.

| Variable | Descripción | Tipo Analítico | Posibles Valores (Muestra) |
| --- | --- | --- | --- |
| `sex` | Género del paciente | Categórica | "F", "M" |
| `age` | Edad en años | Numérica | 32, 55, 44, 58... |
| `weight_kg` | Peso corporal | Numérica | 69.1, 118.7, 108.3, 99.5... |
| `height_m` | Altura en metros | Numérica | 1.71, 1.69, 1.83, 1.80... |
| `height_cm` | Altura en centímetros | Numérica | 171, 169, 183, 186... |
| `bmi` | Índice de Masa Corporal | Numérica | 23.6, 41.6, 26.9, 33.4... |
| `abdominal_circumference_cm` | Perímetro abdominal | Numérica | 86.2, 82.5, 106.7, 96.6... |
| `blood_pressure_mm_hg` | Presión arterial (Cadena original) | Num. Compuesta | "125/79", "139/70", "104/77", "140/83"... |
| `systolic_bp` | Presión arterial sistólica | Numérica | 125, 139, 104, 140... |
| `diastolic_bp` | Presión arterial diastólica | Numérica | 79, 70, 77, 83... |
| `blood_pressure_category` | Clasificación de la presión arterial | Categórica | "Normal", "Elevated", "Hypertension Stage 1", "Hypertension Stage 2" |
| `total_cholesterol_mg_d_l` | Colesterol total en sangre | Numérica | 248, 162, 103, 134... |
| `hdl_mg_d_l` | Colesterol HDL (Alta densidad) | Numérica | 78, 50, 73, 46... |
| `estimated_ldl_mg_d_l` | Colesterol LDL (Baja densidad) | Numérica | 140, 82, 0, 58... |
| `fasting_blood_sugar_mg_d_l` | Glucosa en ayunas | Numérica | 111, 135, 114, 91... |
| `smoking_status` | Tabaquismo activo | Categórica | "N", "Y" |
| `diabetes_status` | Diagnóstico de diabetes | Categórica | "Y", "N" |
| `physical_activity_level` | Intensidad de actividad física | Categórica | "Low", "Moderate", "High" |
| `family_history_of_cvd` | Antecedentes familiares CVD | Categórica | "N", "Y" |
| `waist_to_height_ratio` | Índice cintura-altura | Numérica | 0.504, 0.488, 0.583, 0.537... |
| `cvd_risk_score` | Puntuación de riesgo acumulado | Numérica | 17.93, 20.51, 12.64, 16.36... |
| **`cvd_risk_level`** | **Nivel de riesgo (Target)** | **Categórica** | **"LOW", "INTERMEDIARY", "HIGH"** |


### B. Breast Cancer Wisconsin (Diagnostic)

Este dataset tiee 569 observaciones con 32 variables.Enntre elas se ecuentran el taregt "diagnosis" la cual indica si es malignoo o beningno. luego se encuentran tambien 10 diferentes caractersticas modfologicas observadas en el nucleo en citologias en celulas tumorales. Para cada una de las 10 características morfológicas observadas, el dataset proporciona tres mediciones distintas que conforman las 30 variables continuas:
- Media (_mean): El valor promedio de la característica para todos los núcleos celulares de la imagen.
- Error Estándar (_sd): La variabilidad técnica o desviación estándar de la medición entre los núcleos.
- Peor o Mayor Valor (_worst): El promedio de los tres valores más grandes detectados, lo cual suele ser el indicador clínico más crítico para determinar malignidad.

| Variable | Descripción | Tipo Analítico | Posibles Valores (Muestra) |
| --- | --- | --- | --- |
| `id` | Identificador de la muestra | Categórica | 842302, 842517, 84300903, 84348301... |
| **`diagnosis`** | **Diagnóstico del tumor (Target)** | **Categórica** | **"M" (Malignant), "B" (Benign)** |
| `radius_mean, sd, worst` | Radio (distancia al perímetro) | Numérica | 17.99, 20.57, 19.69, 11.42... |
| `texture_mean, sd, worst` | Textura (desviación escala grises) | Numérica | 10.38, 17.77, 21.25, 20.38... |
| `perimeter_mean, sd, worst` | Perímetro del núcleo | Numérica | 122.8, 132.9, 130, 77.58... |
| `area_mean, sd, worst` | Área del núcleo celular | Numérica | 1001, 1326, 1203, 386.1... |
| `smoothness_mean, sd, worst` | Suavidad (variación de radios) | Numérica | 0.1184, 0.0847, 0.1096, 0.1425... |
| `compactness_mean, sd, worst` | Compacidad nuclear | Numérica | 0.2776, 0.0786, 0.1599, 0.2839... |
| `concavity_mean, sd, worst` | Severidad de hendiduras | Numérica | 0.3001, 0.0869, 0.1974, 0.2414... |
| `concave_points_mean, sd, worst` | Número de hendiduras | Numérica | 0.1471, 0.0701, 0.1279, 0.1052... |
| `symmetry_mean, sd, worst` | Simetría nuclear | Numérica | 0.2419, 0.1812, 0.2069, 0.2597... |
| `fractal_dimension_mean, sd, worst` | Dimensión fractal (irregularidad) | Numérica | 0.0787, 0.0566, 0.0599, 0.0974... |


## 1.4. Pre-filtering

Antes de proceder al análisis estadístico, realizamos una criba de variables basada en el conocimiento del dominio médico y en los objetivos de los modelos que aplicaremos. El objetivo es eliminar variables irrelevantes (ruido) o aquellas que puedan provocar Data Leakage, asegurando que los modelos aprendan patrones biológicos y no reglas matemáticas triviales.

A. Dataset Cardiovascular (CVD)
Eliminamos las siguientes variables por redundancia lógica y riesgo de sobreajustamiento:

cvd_risk_score: Al ser la puntuación numérica de la que deriva directamente nuestro target (cvd_risk_level), su inclusión permitiría al modelo "hacer trampa" (leakage), prediciendo el resultado mediante una fórmula matemática en lugar de aprender los patrones biológicos subyacentes.

blood_pressure_category: Es una categorización simplificada basada en umbrales fijos de presión arterial que ya tenemos desglosados con mayor precisión en las variables sistólica y diastólica.

blood_pressure_mm_hg: Se elimina por ser un dato compuesto de tipo cadena (ej. "125/79"), difícil de procesar algorítmicamente y ya representado de forma atómica por otras dos columnas.

height_cm: Se descarta por redundancia absoluta, manteniendo únicamente la altura en metros (height_m) para el cálculo de proporciones.

Nota: aunque aqui podiramos eliminar aun mas redunancias (p ej la altura o peso porque tenemos ya BMI) las mantendremos de momento para no hacer un pre-filtrado demasiado agresivo y porque las analizaremos mas adelante


B. Breast Cancer Wisconsin (Diagnostic)
En este escenario clínico, la purga es más directa pero igualmente crítica para la validez del modelo:

id: Este atributo es un mero identificador administrativo de la muestra. No posee ninguna carga informativa sobre la morfología celular y, de mantenerse, podría generar un falso aprendizaje si el algoritmo encuentra correlaciones espurias con el número de registro.

```{r}
# Aplicamos la purga
filtered_data <- pre_filtering(cvd_raw, cancer_raw)

# Extraemos los datasets ya limpios
cvd_filtered    <- filtered_data$cvd
cancer_filtered <- filtered_data$cancer
```

## 1.4 Varaible exploration

Pasemos ahora a explorar un poco mas cada variable. Antes de nada, pero, para que R reconozaca correctamente cada variable, transformaremos las categoricas en factors i aseguraraemos que las numericas sean numericas.

```{r}
# 1. Definimos las listas de variables categóricas según nuestro diccionario

# En CVD tenemos un mix de factores demográficos y niveles de riesgo
cols_cvd_cat <- c("sex", "smoking_status", "diabetes_status", 
                  "physical_activity_level", "family_history_of_cvd", 
                  "cvd_risk_level")

# En Breast Cancer, el tipado es directo: solo el diagnóstico es categórico
cols_cancer_cat <- c("diagnosis")

# 2. Aplicación de la función universal de tipado
# Nota: cvd_ready y cancer_ready vienen del paso anterior de pre-filtering
cvd_typed    <- type_data(cvd_filtered, cols_cvd_cat)
cancer_typed <- type_data(cancer_filtered, cols_cancer_cat)
```

Como siguiente paso, para dictaminar nuestra estrategia de limpieza, analizamos el resumen estadístico de ambos conjuntos de datos.

```{r}
# Exploración estadística de los datasets brutos
cat("--- SUMMARY: CARDIOVASCULAR DISEASE (CVD) ---\n")
summary(cvd_typed)

cat("\n--- SUMMARY: HEPATITIS C VIRUS (HCV) ---\n")
summary(cancer_typed)
```

El análisis exploratorio revela cuatro factores clave que condicionan de forma directa la estrategia de preprocesamiento: la presencia de valores atípicos, los valores ausentes, la disparidad de escalas entre variables y el desbalanceo en la variable objetivo. El impacto de cada uno de estos factores depende críticamente del tipo de modelo empleado.

1. Valores atípicos (Outliers)

Vemos indicions de la presencia de outliers en ambos dataets. En general se identifican tanto valores extremos plausibles desde el punto de vista clínico como valores claramente erróneos.

Pongamos algunos ejemplos. El caso más evidente es estimated_ldl_mg_d_l con un valor mínimo de –18.0 mg/dL, lo cual es biológicamente imposible y apunta a un error en el cálculo (p. ej., fórmula de estimación del LDL) o en el registro de los datos. Este tipo de valores debe considerarse inválido y tratarse mediante corrección o eliminación.

Por otro lado, valores como por ejemplo los valores elevados de presión arterial sistólica (p. ej., un máximo de 179 mmHg frente a una media de ~125 mmHg). Aun asi, estos son clinicamente plausibles (hipertensiones severas o episodios agudos) y, en no ser exagerados, se debe valorar mantenerlos ya que pueden aportar informacion util.

En el de Cáncer, tambien vemos posibles outliers. I es que muchas de las variables (por no decir todas) tienen valores de max que estan bastante alejados de la mediana. Un ejemplo claro es el area_worst que presenta un máximo de 4254.0 frente a una media de 880.6. Aun asi, consideramos que la mayororia de estos casos parecen ser plausibles biologicamente: celulas que debido a la malignidad o cambios reactivos se vuelvem muy grandes i salen de la media. Aquneu habra que analizarlosen detlle, no creemos que deban ser eliminados,

En cualqier caso, cada uno de estos valores extremos deberia de ser identificado i avaluado individualmente.

2. Valores ausentes (NaNs)

En el CDV, todas las variables numéricas presentan entre 64 y 82 valores ausentes (≈5% del conjunto de datos), lo que sugiere un problema de calidad de datos relativamente sistemático y no meramente anecdótico. La eliminación de filas con valores ausentes supondría perder alrededor de un centenar de individuos, con el consiguiente riesgo de sesgo de selección y pérdida de representatividad de la muestra. Parasolucionarlo, se puede usar algnu tipo de imputacion/interpolacoin.

En el cancer el conjunto de datos no presenta valores ausentes, lo que simplifica considerablemente el pipeline de preprocesamiento. Si que es verdad que aparecen variables con min = 0, lo que poodira sugerir valores nulos identificados como un 0... (ya que estamos hablando de medidadas). Sin embargo, si nos fijamos, los valores de 0.0 aparecen principalmente en:

concavity_mean / sd / worst

concave_points_mean / sd / worst

En el dataset de Breast Cancer, estas variables miden la presencia de hendiduras en el contorno del núcleo. Una célula sana suele ser perfectamente elíptica o circular (bordes lisos). Si una célula es perfectamente lisa, la "severidad de las porciones cóncavas" (concavity) es literalmente cero. Por tanto, un 0.0 en concavidad no es un dato ausente, es un dato clínico muy valioso: nos está diciendo que ese núcleo es perfectamente regular.

3. Disparidad de escalas (requisitos de escalado)

Existe una heterogeneidad extrema en las escalas de las variables. Por ejemplo, en CDV coexisten variables como waist_to_height_ratio (~0.5) con otras como total_cholesterol (~200 mg/dL). Asi insmo, en el de cancer el contraste es aún más pronunciado; por ejemplo mientras que métricas como smoothness_mean oscilan en valores minúsculos (0.05 - 0.16), el area_worst alcanza los miles (4254). Sin un escalado previo, las variables con mayor magnitud dominarían injustamente los cálculos de distancia.

4. Desbalanceo en la variable objetivo

En el escenario cancer Presenta una distribución relativamente equilibrada (357 benignos frente a 212 malignos), lo que permite un entrenamiento estable. No obstante, en CVD la variable objetivo de riesgo cardiovascular muestra un desbalanceo notable (p. ej., clase low claramente minoritaria frente a intermediate y high).

El impacto de estos factores depende mucho del tipo de modelo:

I. Modelos no supervisados (clustering)

En K-means, los valores extremos desplazan los centroides y pueden distorsionar la estructura de los clústeres, agrupando individuos disímiles por la influencia de una sola variable extrema. DBSCAN es más robusto al etiquetar puntos aislados como ruido, aunque valores extremos situados entre regiones densas podrían actuar como “puentes” artificiales y fusionar grupos clínicamente distintos.

 Asi mismo, la presencia de valores ausentes impide el cálculo directo de distancias (euclídea, Manhattan). Sin imputación, los individuos con NaNs quedarían excluidos del análisis, reduciendo el tamaño muestral y sesgando la detección de estructuras latentes. pOR OTRO LADO, Sin estandarización, las variables de mayor magnitud dominarían completamente el proceso de clustering, invalidando la interpretación biológica de los grupos resultantes.

Porotro lado, la disparidad ground truth genera un sesgo geométrico de tamaño, donde los clústeres más numerosos (como el grupo HIGH) actúan como "aspiradoras" estadísticas que absorben a los puntos de las clases minoritarias (LOW) para minimizar la varianza global. En algoritmos como K-means, esto desplaza los centroides hacia la masa de datos más densa, mientras que en DBSCAN, la diferencia de densidad puede provocar que el grupo mayoritario se fusione con otros o que el minoritario sea descartado erróneamente como ruido. Esto es dificil de arreglar asi que deberemos aceptar este sesgo i entrenar al mdelo con esto


II. Modelos supervisados (clasificación)

Los arboles de decisión (C5.0) son intrínsecamente invariantes a la escala y relativamente robustos frente a outliers, ya que realizan particiones basadas en umbrales (p. ej., systolic_bp > 150). Además, mantener las escalas originales facilita la interpretabilidad clínica de las reglas. No obstante, la presencia de valores ausentes puede degradar el rendimiento, aunque algunos algoritmos implementan mecanismos de gestión parcial de NaNs.

Random Forest comparte las ventajas de los árboles individuales en cuanto a robustez frente a escalas y valores extremos. Aun así, el tratamiento explícito de valores ausentes sigue siendo recomendable para maximizar la estabilidad y el rendimiento del modelo.

Finalmente, a diferencia de los árboles, la regrresion logistica es altamente sensible a la escala y a los outliers. Al ser una combinación lineal de predictores, una variable con magnitudes de miles eclipsaría totalmente a una de escala decimal si no se aplica estandarización. Asimismo, los outliers pueden "tirar" de la curva logística y desplazar la frontera de decisión, afectando la precisión del clasificador.

En todos estos modelos, el fallo crítico es el desbalanceo en CVD. Los algoritmos tienden a optimizar la exactitud global, lo que desplaza la frontera hacia las clases mayoritarias, bajando la sensibilidad (recall) de la clase minoritaria e incrementando el riesgo de infra-detección del grupo LOW. A diferencia del clustering, aquí podemos paliar el efecto mediante matrices de costo o métricas F1 por grupo.

Todos estos factores que hemos visto se tendran en cuenta en siguientes fases.

## 1.5 Valores extremos y outliers

Como dijimos, los outliers pueden afectar a nuestros modelos, principalmente a los de clasificacion.

There are many diferents maneras de descubrir un outlier. Una de las mas comunes, que usaremos aqui, es mediante el rant IQT. Mathematically, the IQR is calculated as:$$IQR = Q_3 - Q_1$$Where $Q_1$ is the 25th percentile and $Q_3$ is the 75th percentile. A data point is statistically considered an outlier if it falls outside the following boundaries:Lower Bound: $Q_1 - 1.5 \times IQR$Upper Bound: $Q_3 + 1.5 \times IQR$

Nosotros represetnaremos esto In a Boxplot, these points appear as individual dots beyond the "whiskers" or tails, allowing us to quickly spot biological impossibilities like our previously noted Hallux measurement. 

Cabe aclarar, pero, que no deben ser directamente definidos como outliers. cada uno debera ser revisado segun el contexto (ya que pueden representar na realidad de un paciente que se deba tener en cuenta ya que representa un limite o de un grupo especifico).

Empecemos con el grafico de CVD:

```{r}
analyze_outliers(cvd_typed, "CVD Dataset")
```
L'anàlisi mitjançant el mètode IQR ens indica que la presència d'outliers és mínima. Les observacions detectades es categoritzen com a outliers clínics i no com a errors de registre; són casos reals i plausibles que s'allunyen de la norma. Son 3 en waist_to_height_ratio i reflecteixen individus amb una circumferència de cintura predominant, però dins d'un rang biològicament lògic. Atès que no s'allunyen excessivament dels "bigotis" (whiskers) del boxplot i aporten informació valuosa sobre casos crítics, es decideix mantenir-los.

No obstant això, aquesta anàlisi posa de manifest les limitacions del mètode estadístic IQR: el sistema no ha detectat el o els valors negatius en estimated_ldl_mg_d_l que vam identificar prèviament. Això ens recorda que la detecció automàtica és una eina d'ajuda, però l'analista sempre ha d'inspeccionar tant els punts fora de rang (per confirmar si són erronis) com els valors dins del rang (per si són biològicament impossibles).

Asi, en este dataset, simplemente ebemos asegurar que el ldl sea psibivo. Al ser outliers proviinientes de errores de caluclo/introducicon del  valor, probalbemente seran pocos outleirs (incluso seguramente solo uno). Por tanto,no necsitamos metodos avanzados pra tratarlo: con la simple susitucion por la mediana nos sirve:

```{r}
# Aplicamos la corrección de errores lógicos (como el LDL negativo)
cvd_no_out <- fix_logical_errors(cvd_typed)
```

Pasemos ahora a examinar el dataset de cancer:

```{r}
analyze_outliers(cancer_typed, "Breast Cancer Dataset")
```


Todas las variables presentan outliers.¿Cómo debemos interpretar esta "lluvia" de outliers?
- En oncología, la malignidad se define precisamente por la aberración. Una célula cancerígena es, por definición, un "outlier" de una célula sana. Los valores extremadamente altos en area o perimeter representan núcleos celulares masivos, que son los indicadores más fiables de un tumor maligno.
- Revisando el summary, vemos que todas las variabls tienen sentido. No hay ninguna que muestre ni un minimo o un maximo NO plausible. Por ejemplo, area_sd, con una mean de 40.33 i nu max de 542.2, represetna un otuler estadistico claro, pero no es implausible en la realidad ya que hablamos justo de cambios reacivos i tumorales que se salen de la media. Aunque estadísticamente parece un error, en una citología por PAAF esto representa un grupo de células con núcleos hipertróficos, algo común en estadios avanzados.
- Al ser mediciones automáticas de imágenes digitalizadas, el riesgo de "error de dedo" al escribir es casi nulo. Los valores son reales.

Por tanto, so eliminaremos los outliers en el dataset de Cáncer. Si lo hiciéramos, estaríamos "limpiando" precisamente los casos que el modelo debe aprender a detectar como Malignos. Estaríamos eliminando la señal de la enfermedad para quedarnos solo con el ruido de la salud.

Esto remarca de nuevo las limitaciones de usar metorodos como el metodo IQT unicamente sin revisar los resultados. Quizas en variables que usan una distrbucion gussiana perfecta es idal, pero en la vida real i sobretodo en datos clinicos, odnde muchas veces buscamos lo "excepcional" este se debe usar solo como una ayuda de deteccion, y siempre revisar i validar los outlers a mano.

## 1.6 Valores absentes

Como vimos, los valores absentes pueden afectar a nuestros modelos.Aqui los trataremos.

Como vimos en revisar el summary, soolo el dataset CVD tiene valores absentes. Si nos fijamos bien, este es un error sistematico (en casi todas las numercas) i suele rondar el 5% respecto al total. La eliminacion es una opcin, pero no la que queremos ya que se elminaria un numero de valores que pueden ser usados.

Afortunadamente, ninguna categoria contiene na, de manera qe podemos recurrir a metodos numericos. hemos decidido usar el kk¡-nearest neightborus (knn). A diferencia de la imputación por la media o la mediana, que trata cada variable de forma aislada y reduce artificialmente la varianza del dataset, el método k-nearest neighbors (KNN) utiliza la estructura global de los datos para estimar los valores faltantes. Su funcionamiento se basa en la proximidad multidimensional: para cada paciente que presenta un valor ausente (un "hueco" en sus datos), el algoritmo busca en el resto del dataset a los $k$ individuos más similares (sus "vecinos") basándose en las variables que sí tenemos disponibles.Una vez identificados estos vecinos más cercanos —normalmente utilizando la distancia euclidiana—, el algoritmo calcula la media ponderada de sus valores para esa variable específica y la asigna al espacio vacío. Este enfoque es el gold standard en datos clínicos porque preserva las correlaciones entre variables: si un paciente tiene una presión arterial alta y un BMI elevado, el KNN le asignará un valor de colesterol coherente con ese perfil metabólico, en lugar de un promedio genérico. Al representar el 5% de la muestra, este método nos permite recuperar la integridad del dataset sin sesgar las distribuciones originales, preparándolo para un clustering mucho más preciso.


```{r}
# Aplicamos la imputación KNN al dataset de CVD
cvd_no_na <- impute_data_knn(cvd_no_out, k = 5)

# Comprobación de seguridad: ¿Quedan NAs?
if(!anyNA(cvd_no_na)) {
  message("Dataset CVD imputado correctamente: 0 valores ausentes.")
}
```

## 1.7 Sanity check

Una bona pràctica en l'auditoria de dades clíniques consisteix a verificar la coherència de les variables derivades. Quan disposem de les variables precursores (com el pes i l'alçada per al BMI), és fonamental validar que el càlcul original és correcte abans de procedir a qualsevol anàlisi estadística avançada. Tot i que el dataset presenta diverses mètriques calculades, només disposem dels precursors complets per a dues d'elles en el dataset CVD: el BMI ($kg/m^2$) i el Waist-to-Height Ratio ($cintura/alçada$).El següent bloc de codi analitza el percentatge d'incongruències matemàtiques (assumint que les variables primàries són les correctes seguint la Llei de Propagació d'Errors) i, posteriorment, recalcula aquests valors per garantir la integritat del dataset:

```{r}
# Aplicamos la auditoría para asegurar que BMI y Ratio son 100% precisos
cvd_sanited <- audit_and_fix_integrity(cvd_no_na)
```

Veiem que ambdues metriques tenen inconcruencies (en especial bmi amb un ~90%). En la formula aplicada s'ha establert una tolerància d'error molt estricta (0.01). Això explica per què observem un percentatge d'error tan elevat. Si aguemntem la tolerància (per exemple, a 0.2) observem comportaments dispars: mentre que en el Waist-to-Height Ratio l'error es residualitza fins a un insignificant 0,13% —confirmant que les discrepàncies eren mers arrodoniments en la captura original—, en el BMI persisteix un 43,1% d'incongruències. Això indica la presència d'errors de càlcul o de registre que van més enllà de la precisió decimal

Hem optat per mantenir el criteri de correcció basat en la tolerància estricta per garantir la màxima precisió matemàtica i eliminar qualsevol rastre de "soroll" en les dades. Amb aquesta intervenció, no només esmenem els errors estructurals detectats en el BMI, sinó que homogenitzem tot el dataset sota un càlcul únic i precís, minimitzant la propagació d'errors cap a les fases de modelització.

## 1.8. Standarization i division del dataset

Tras haber limpiado, imputado y corregido la integridad de los datos, debemos adaptar su estructura a los requisitos de los modelos que emplearemos. No todos los algoritmos "ven" los datos de la misma forma, por lo que crearemos tres variantes de nuestros datasets para optimizar el rendimiento de cada familia de modelos.

algoritme ens requereix fer una cosa o un altre.

Para empezar, Como observamos en el análisis exploratorio, variables como el area_worst (miles) y smoothness_mean (decimales) conviven en escalas totalmente dispares. En modelos basados en distancias o geometría del espacio de características (como kmeans, dbcan o regreison logistica), esta disparidad provoca que las variables de mayor magnitud dominen el proceso de aprendizaje, relegando a un segundo plano variables clínicamente relevantes pero numéricamente pequeñas. Por tanto, el escalado e+s imprescindible en estos contextos. 
No obstante, los modelos de arboles no se ven perjudicados por esto (al ser modelos con reglas de corte, les es indiferent l'escala) i de hecho se ven favorecidos de no tocarlas (ya que las reglas se hacen mas interpretables, no es lo mismo decir que el riesgo atumenta en 200 de ldl que en 0,9).

Para solucionar esto, aplicaremos la estandarización Z-score en los contextos que lo requieran. Esta técnica transforma cada valor $x$ restándole la media $\mu$ y dividiéndolo por la desviación estándar $\sigma$:$$z = \frac{x - \mu}{\sigma}$$El resultado es un dataset donde cada variable tiene una media de 0 y una desviación estándar de 1, permitiendo que todas "compitan" en igualdad de condiciones en el espacio vectorial.

Por otro lado, En el dataset CVD, contamos con factores cualitativos (sexo, tabaquismo, etc.). Para el Aprendizaje No Supervisado (K-means y DBSCAN), estas variables presentan un problema: no se puede calcular una distancia euclídea real entre "FUMADOR" y "NO FUMADOR". Intentar codificarlas numéricamente (1 y 2) induciría un error biológico, ya que el algoritmo interpretaría que existe una magnitud o un orden que no es real, distorsionando la formación de los clústeres. Por tanto, para el clustering, nos quedaremos exclusivamente con el núcleo numérico.

En cambio, los Modelos Supervisados sí pueden integrar estas etiquetas:

- Árboles (C5.0 y Random Forest):  Son capaces de incorporar de manera nativa las categoricas sin necesidad de tratarlas (codificarlas). ... (EXPLICA BREVEMENTE COMO LAS TRATA I ES CAPAZ DE CREAR RAMIFICACIONES CON ELLAS)
- Regresión Logística: diferencia de los árboles, este modelo es puramente matemático y no admite texto. Sin embargo el algoritmo es capaz de realizar un Dummy Coding  (one-hot encoding) Por ejemplo, si una variable categórica tiene tres niveles (p. ej., “No fumador”, “Exfumador” y “Fumador”), la regresión logística elige una categoría de referencia (por ejemplo, “No fumador”) y crea internamente dos variables binarias. El modelo estima entonces cómo cambia la probabilidad del evento al pertenecer a cada una de las otras categorías en comparación con la categoría de referencia.

Por tanto, en esta fase, crearemos tres variantes por cada escenario médico (CVD y Cáncer), sumando un total de 6 datasets listos para la acción:

1. Datasets para Clustering (_cluster)
Contenido: Solo variables numéricas.
Tratamiento: Estandarización Z-score completa.
Objetivo: K-means y DBSCAN (Geometría pura).

2. Datasets para Árboles (_trees)
Contenido: Variables numéricas + Variables categóricas (como factores).
Tratamiento: Escala original (sin estandarizar) para máxima interpretabilidad.
Objetivo: C5.0 y Random Forest.

3. Datasets para Regresión Logística (_logistic)
Contenido: Variables numéricas + Variables categóricas.
Tratamiento: Solo las variables numéricas son estandarizadas (Z-score); las categóricas se mantienen como factores.
Objetivo: Regresión Logística (Equilibrio entre escala y categorías).














```{r}
# Estandarizamos los tres datasets
cvd_scaled <- standardize_data(cvd_sanited)
hcv_scaled <- standardize_data(hcv_log)

# Verificación rápida: la sd de cualquier numérica debe ser ~0 y la sd = 1
mean(cvd_scaled$age)
sd(cvd_scaled$age)
```


```{r}
# Despachamos los datasets usando los objetos que ya hemos procesado previamente
final_assets <- partition_medical_datasets(cvd_scaled, hcv_scaled, cvd_sanited, hcv_typed)

# Nomenclatura final para el resto del proyecto
cvd_cluster <- final_assets$cvd_cluster
hcv_cluster <- final_assets$hcv_cluster
cvd_super   <- final_assets$cvd_super
hcv_super   <- final_assets$hcv_super
```

## 1.10 Redundancias

L'objectiu d'aquesta fase és tractar les variables redundants: aquelles que aporten informació duplicada o extremadament correlacionada, generant soroll i risc de sobreajustament (overfitting). No obstant això, la nostra estratègia per gestionar-les dependrà de la naturalesa del model:

- Per als datasets dels models no supervisats: Els models de clustering basats en distàncies són molt sensibles a la redundància; si dues variables diuen el mateix, el seu pes en el càlcul de la distància es duplica artificialment, esbiaixant els clústers. Per solucionar-ho, aplicarem una Anàlisi de Components Principals (PCA).
El PCA transforma les variables originals en unes de noves anomenades Components Principals, que són ortogonals entre si (no col·lineals) i capturen la màxima variància possible. Això ens permet reduir la dimensionalitat eliminant el soroll de forma automàtica.

- Per als datasets dels models supervisats: En els models supervisats, no és recomanable aplicar PCA, ja que perdríem la interpretabilitat clínica: les regles de l'arbre deixarien d'expressar-se en unitats reals (com el pes o el colesterol) per fer-ho en components abstractes. Tot i que els arbres són més robusts davant la col·linealitat, mantenir variables redundants pot fragmentar la importància dels atributs i fer l'arbre innecessàriament complex. Per tant, realitzarem una eliminació manual basada en el coeficient de correlació i el coneixement del domini.

### Matriu de Correlació

Comencemos el tratamiento manual para los datasets para metodos supervisados.

Per identificar quines variables són candidates a ser eliminades en els models supervisats, utilitzarem el coeficient de correlació de Pearson. Aquest mètode mesura el grau de relació lineal entre dos atributs en un rang de $[-1, 1]$.Establirem un llindar crític de $|r| > 0.85$. Si una parella de variables supera aquest valor, es consideraran redundants i optarem per mantenir-ne només una (la que tingui més valor clínic o sigui més fàcil de mesurar), simplificant així el model i millorant la seva parsimònia.


```{r, fig.width=10, fig.height=8}
# Llamada directa en una sola línea por dataset
plot_correlation_matrix(cvd_super, "CVD: Análisis de Redundancias")
plot_correlation_matrix(hcv_super, "HCV: Análisis de Redundancias")
```

Empezando por CVD, vemos que hay redundancias claras) (>0,85). Una obvia es heigh_m i height_cm con un 0.98 (sorprene que no sea de 100% por lo que hay incongruencias minimas).Tabueb vmeos una correlacion alta entre estimated ldl i total cholesterol (0.93).  Tambien vemos una relcion clara entre waist_to_height_Ratio i abdomila_circumference_cm (0.9) claro debido a que es parte de su formula, pero solo de -0,4 con las dos de height (aunque tambien forman parte).  tambien corr alta entre bmi i weight kg (0.88), normal al formar parte del calcuoo, pero sorprende que ambas height tienen solo un -0,44.
El resto de corr tienen valores bajos (en general, aunq con alguyna excpecion, por debajo de 0.1)

En base a lo anteiror, i en base tambien al conocimiento de dominio, eliminaremos:
- height_m, height:cn i weight: al estar reconocidas en bmi
- abdomila_circumference_cm al estar ya en weist to heist ratio
- nos quedaremos con total_cholesterol i eliminaremos ldl (ya que, al ser este un estimado, es mas propenso a errores que el original)

En el de HCV no veiem cap corr >0.85 (son todsa inferiores a 0.1). Tam,poco recomocemos ninguna que por cnociiento del doiinio deba ser redundnte (porq ya se vea reconocida por otras). Por tanto aqui no debemos eliminar niguna.


```{r}
# Aplicamos la eliminación de variables redundantes según nuestro análisis
optimized_assets <- handle_medical_redundancies(cvd_super, hcv_super)

# Actualizamos nuestros activos para supervisados
cvd_super <- optimized_assets$cvd
hcv_super <- optimized_assets$hcv
```

Esos serian, en principio, los datasets finales ya preparados para modelos supervisados.

### PCA

El Análisis de Componentes Principales (PCA) es una técnica de transformación lineal que proyecta los datos originales hacia un nuevo espacio de variables ortogonales denominadas Componentes Principales, ordenadas según la varianza que capturan.

1. Centrado de los datos: Partimos de la matriz de datos original $X \in \mathbb{R}^{n \times p}$. Para asegurar que el análisis se centre en la variabilidad y no en el desplazamiento, restamos la media $\mu$ de cada columna:$$X_{c} = X - \mu$$ Donde:$X_c$: es la matriz de datos centrados. (Nota: Dado que nuestros datos ya han sido estandarizados por Z-score, la media $\mu$ ya es 0).

2. Matriz de covarianzas: Calculamos la matriz de covarianzas $S$ para capturar cómo las variables se relacionan entre sí:$$S = \frac{1}{n-1} X_{c}^{\top} X_{c}$$
donde $X_c^\top$: es la matriz centrada transpuesta i $n$: es el número total de observaciones (pacientes).

3. Descomposición en autovalores y autovectors: Resolvemos la ecuación característica para encontrar las direcciones donde los datos están más dispersos:$$S w = \lambda w$$Donde:$w$: son los autovectores (o loadings), que definen la dirección y el peso de cada variable original en los nuevos ejes.$\lambda$: son los autovalores, que representan la magnitud de la varianza explicada en cada dirección.

4. Proyección al nuevo espacio (Scores): Finalmente, calculamos las nuevas coordenadas de cada paciente (llamadas scores) proyectando los datos sobre los autovectores seleccionados ($k$):$$Z = X_{c} W_{k}$$Donde:$Z$: es la nueva matriz de datos transformados (coordenadas de los pacientes en el nuevo espacio).$W_k$: es la matriz que contiene los $k$ autovectores principales.

Para decidir cuántas componentes conservar (cuantos k autovectores selccionamos en el paso anteiro), calculamos la Proporción de Varianza Explicada (PVE):$$\text{PVE}_{j} = \frac{\lambda_{j}}{\sum_{i=1}^{p} \lambda_{i}}$$Donde:$\text{PVE}_j$: es el porcentaje de información que retiene la componente $j$.$\lambda_j$: es el autovalor de la componente actual.$\sum \lambda_i$: es la suma de todos los autovalores (varianza total).


```{r}
# Ejecutamos el motor de PCA manual para ambos escenarios
res_pca_cvd <- perform_manual_pca(cvd_cluster, "CVD: Variancia por Componente")
res_pca_hcv <- perform_manual_pca(hcv_cluster, "HCV: Variancia por Componente")

# Verificamos los nuevos valores (Scores) del primer paciente de CVD
# Estos son los valores que usará el clustering
print("Nuevas coordenadas (Scores) del Paciente 1 en CVD:")
head(res_pca_cvd$scores[1, 1:5])
```


## 1.11. representaicon visual

distribuciones de numaercias segun target, entre otroxs
